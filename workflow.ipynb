{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855677f2",
   "metadata": {},
   "source": [
    "# First step\n",
    "We use demucs to separate vocals\n",
    "\n",
    "# Second\n",
    "\n",
    "We get a rough timestamp(using kotoba-tech/kotoba-whisper-v2.0-faster, 4m processing for a 2h movie) and an accurate transcript without timestamp(using [amine whisper](https://huggingface.co/litagin/anime-whisper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0585bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "import time # 引入time模块，用于在API调用之间添加延迟\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c601ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio(input_video: str,\n",
    "                  sample_rate: int = 16000,\n",
    "                  channels: int = 1,\n",
    "                  codec: str = \"pcm_s16le\") -> Path:\n",
    "    \"\"\"\n",
    "    从任意视频文件中提取音频，输出 WAV 并返回输出文件路径。\n",
    "    文件名：保留原始 stem，后缀改为 .wav\n",
    "    \"\"\"\n",
    "    in_path = Path(input_video)\n",
    "    stem = in_path.stem\n",
    "    audio_output = in_path.with_name(f\"{stem}.wav\")\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-y\",  # 如果输出已存在则覆盖\n",
    "        \"-i\", str(in_path),\n",
    "        \"-vn\",\n",
    "        \"-acodec\", codec,\n",
    "        \"-ar\", str(sample_rate),\n",
    "        \"-ac\", str(channels),\n",
    "        str(audio_output)\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    return audio_output\n",
    "\n",
    "\n",
    "def separate_vocals(audio_input: Path,\n",
    "                    stems: str = \"vocals\",\n",
    "                    model: str = \"htdemucs\") -> None:\n",
    "    \"\"\"\n",
    "    对提取后的音频做人声分离，两声道模式下只保留 vocals。\n",
    "    Demucs 会在当前目录下创建一个 separated/<model>/<stem> 文件夹。\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"demucs\",\n",
    "        f\"--two-stems={stems}\",\n",
    "        \"-n\", model,\n",
    "        str(audio_input)\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "\n",
    "video_path = Path(\"D:/Downloads/.mp4\")\n",
    "\n",
    "# 提取音频，自动生成 your_movie.wav\n",
    "wav_path = extract_audio(video_path,\n",
    "                            sample_rate=16000,\n",
    "                            channels=1)\n",
    "\n",
    "# 基于 your_movie.wav 分离人声\n",
    "demucs_model = \"htdemucs\" \n",
    "separate_vocals(wav_path,\n",
    "                stems=\"vocals\",\n",
    "                model=demucs_model)\n",
    "\n",
    "# 分离后的人声文件路径\n",
    "vocals_path = Path(f\"separated/{demucs_model}/{wav_path.stem}/vocals.wav\")\\\n",
    "\n",
    "print(f\"分离后的人声文件路径: {vocals_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17cc4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"language\": \"Japanese\",\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "}\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"./models/anime\",\n",
    "    device=\"cuda\",\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "    chunk_length_s=30.0,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "result = pipe(str(vocals_path), generate_kwargs=generate_kwargs)\n",
    "\n",
    "os.makedirs(\"transcripts\", exist_ok=True)\n",
    "with open(f\"transcripts/{wav_path.stem}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "out_dir = Path(f\"timestamps/{wav_path.stem}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "cmd = [\n",
    "    \"whisperx\", str(vocals_path),\n",
    "    \"--model\", \"kotoba-tech/kotoba-whisper-v2.0-faster\",\n",
    "    \"--device\", \"cuda\",\n",
    "    \"--language\", \"ja\",\n",
    "    \"--vad_method\", \"silero\",\n",
    "    \"--chunk_size\", \"6\",\n",
    "    \"--batch_size\", \"16\",\n",
    "    # \"--output_format\", \"json\",\n",
    "    \"--highlight_words\", \"True\",\n",
    "    \"--output_dir\", str(out_dir)\n",
    "]\n",
    "subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb04fcb",
   "metadata": {},
   "source": [
    "# Third step\n",
    "Use openai API to align the trancription to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127349ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 处理批次 1/13 ===\n",
      " -> 正在定位精确文本片段…\n",
      "检测到 style_guide 文件，正在加载...\n",
      "正在调用OpenAI API处理当前批次，请稍候...\n",
      "  成功获得 30 条字幕\n",
      "\n",
      "=== 处理批次 2/13 ===\n",
      " -> 正在定位精确文本片段…\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "\n",
    "import time # 引入time模块，用于在API调用之间添加延迟\n",
    "from dotenv import load_dotenv\n",
    "import pysrt\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "\n",
    "# --- 配置 ---\n",
    "# 确保你已经设置了环境变量或在代码中提供了API密钥\n",
    "# 推荐使用 gpt-4o 或 gpt-4-turbo 以获得最佳性能和成本效益\n",
    "# ---------- 配置 ----------\n",
    "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY2\"]) # Ensure you have set your OpenAI API key in the environment variable or .env file\n",
    "\n",
    "wav_path = Path(\"D:/Downloads/mida-198.wav\")\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "MODEL_FOR_ALIGNMENT = \"gpt-4.1\"  # 使用 o3 模型进行对齐和翻译\n",
    "MODEL_FOR_CHUNKING  = \"gpt-4.1\"\n",
    "\n",
    "# 对齐‑翻译结果的 JSON Schema\n",
    "ALIGN_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"segments\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"start\":           {\"type\": \"number\"},\n",
    "                    \"end\":             {\"type\": \"number\"},\n",
    "                    \"original_text\":   {\"type\": \"string\"},\n",
    "                    \"translated_text\": {\"type\": \"string\"},\n",
    "                },\n",
    "                # 这里必须列出所有属性\n",
    "                \"required\": [\"start\", \"end\", \"original_text\", \"translated_text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"segments\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# ---------- 辅助函数 ----------\n",
    "def load_data(json_path, accurate_text_path):\n",
    "    \"\"\"加载 JSON 时间戳数据和精确文本\"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            timing_data = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"加载 JSON 失败: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        with open(accurate_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            accurate_text = f.read()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"加载精确文本失败: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    return timing_data, accurate_text\n",
    "\n",
    "\n",
    "def format_timing_data_for_prompt(segments_batch):\n",
    "    lines = []\n",
    "    for seg in segments_batch:\n",
    "        lines.append(f'  - 时间: {seg[\"start\"]:.3f} - {seg[\"end\"]:.3f}, 文本: \"{seg[\"text\"]}\"')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def srt_to_segments_json(srt_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    将 SRT 文件解析为 { \"segments\": [ { start, end, text }, ... ] } 的结构。\n",
    "    \"\"\"\n",
    "    subs = pysrt.open(srt_path, encoding=\"utf-8\")\n",
    "    segments = []\n",
    "    for sub in subs:\n",
    "        # pysrt 的 start/end 是 SubRipTime 对象，转换为秒数\n",
    "        start_sec = (\n",
    "            sub.start.hours * 3600\n",
    "            + sub.start.minutes * 60\n",
    "            + sub.start.seconds\n",
    "            + sub.start.milliseconds / 1000.0\n",
    "        )\n",
    "        end_sec = (\n",
    "            sub.end.hours * 3600\n",
    "            + sub.end.minutes * 60\n",
    "            + sub.end.seconds\n",
    "            + sub.end.milliseconds / 1000.0\n",
    "        )\n",
    "        # 把内部换行替换成空格\n",
    "        text = sub.text.replace(\"\\n\", \" \")\n",
    "        segments.append({\n",
    "            \"start\": round(start_sec, 3),\n",
    "            \"end\":   round(end_sec,   3),\n",
    "            \"text\":  text\n",
    "        })\n",
    "\n",
    "    result = {\"segments\": segments}\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "# --- 核心API调用函数 ---\n",
    "\n",
    "def extract_relevant_chunk_with_api(full_accurate_text, segment_batch):\n",
    "    \"\"\"用 Responses API 从长文本中定位与当前批次对应的片段\"\"\"\n",
    "    instructions = (\n",
    "        \"\"\"\n",
    "        你是一个精通中文和日文的双语内容定位专家。\n",
    "        你的唯一任务是：根据一段“参考中文文本”的含义，在一部“完整日文文本”中，找到并提取出与之语义完全对应的原始日文片段。\n",
    "\n",
    "        # 你的工作流程：\n",
    "        1.  **理解中文**：仔细阅读并完全理解“参考中文文本”所表达的核心意思。\n",
    "        2.  **定位日文**：在“完整日文文本”中地毯式搜索，找到与中文含义完全匹配的日文句子或段落。\n",
    "        3.  **精确提取**：完整地提取出你找到的那部分日文原文。为了保证上下文的连贯性，你可以适度地包含前后相邻的几个词或语句，切记只能多不能遗漏。\n",
    "        4.  **纯净输出**：你的输出**必须**是纯粹的、未经修改的日文文本，除非原文有乱码非日文字符。**绝对不要**包含任何解释、标题、标签、引号或任何非日文内容。\n",
    "\n",
    "        ---\n",
    "        【一个完整的处理范例】\n",
    "        [完整日文文本]:\n",
    "        ...失礼します。ちょっとインタビューさせていただきたいなと思います。はい、夏目響です。デビューして何年目ぐらいですか？5周年になりましたので6年目です。いろんな役が演じてきて、セックス感も変わってきたんじゃないですか...\n",
    "\n",
    "        [参考中文文本]:\n",
    "        是的，我是夏目响。你出道几年了？因为已经5周年了，所以是第6年。\n",
    "\n",
    "        [输出]:\n",
    "        はい、夏目響です。デビューして何年目ぐらいですか？5周年になりましたので6年目です。\n",
    "        ---\n",
    "\n",
    "        现在，请严格遵循以上规则和范例，处理下面的实际任务。\n",
    "        \"\"\"\n",
    "    )\n",
    "    reference_text = \" \".join([s[\"text\"] for s in segment_batch])\n",
    "    user_input = (\n",
    "        f\"这是“完整文本”：\\n---\\n{full_accurate_text}\\n---\\n\\n\"\n",
    "        f\"请提取与下列“参考文本”对应的部分：\\n---\\n{reference_text}\\n---\"\n",
    "    )\n",
    "\n",
    "    print(\" -> 正在定位精确文本片段…\")\n",
    "    try:\n",
    "        rsp = client.responses.create(\n",
    "            model=MODEL_FOR_CHUNKING,\n",
    "            instructions=instructions,\n",
    "            input=user_input,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        return rsp.output_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"定位失败: {e}\")\n",
    "        return None\n",
    "\n",
    "def align_and_translate(formatted_timings, accurate_text_chunk):\n",
    "    \"\"\"对齐 + 翻译，要求模型按 JSON Schema 输出\"\"\"\n",
    "    if os.path.exists(\"style_guide\"):\n",
    "        print(\"检测到 style_guide 文件，正在加载...\")\n",
    "        with open(\"style_guide\", \"r\", encoding=\"utf-8\") as f:\n",
    "            style_guide = f.read().strip()\n",
    "    else:\n",
    "        print(\"未找到 style_guide 文件，使用默认空字符串\")\n",
    "        style_guide = \" \"\n",
    "    instructions = f\"\"\"\n",
    "    # 角色和最终目标\n",
    "    你是一个AI字幕生成专家。你的任务是融合两份不同语言的源材料，生成一个**帧级别精确**、**文本内容高质量**、且**翻译流畅**的中文JSON字幕文件。\n",
    "\n",
    "    # 输入定义\n",
    "    1.  **`带时间戳的粗糙中文转录本` (Rough Chinese Transcript)**: 这是你的**时间基准**和**定位锚点**。它的时间戳是绝对可信的。它的中文文本是机器快翻，质量不高，**仅用于定位**，但最终会原样放入输出的 `original_text` 字段。\n",
    "    2.  **`高精度日文转录本` (Precise Japanese Transcript)**: 这是你进行高质量翻译的**唯一真实文本来源**。它的文本内容是高度可信的，但没有时间信息。\n",
    "\n",
    "    # 核心原则 (Golden Rules)\n",
    "    1.  **时间戳权威**: `Rough Chinese Transcript` 的 `start` 和 `end` 时间戳必须被完美保留，不得有任何修改。\n",
    "    2.  **文本选择与回退逻辑 (最重要)**:\n",
    "        - **优先路径**: 基于 `Rough Chinese Transcript` 中某一段中文的**含义**，在 `Precise Japanese Transcript` 中定位到对应的**日文原文**。如果该日文原文清晰、完整且有意义，就**必须使用这段日文**来进行全新的、高质量的中文翻译。\n",
    "        - **回退路径**: 如果在 `Precise Japanese Transcript` 中定位到的对应日文部分是无意义的语气词、AI幻觉或与上下文严重不符，那么你**必须放弃这段劣质日文**。在这种情况下，你的最终翻译结果 (`translated_text`) 应该直接使用 `Rough Chinese Transcript` 中对应的中文文本，可以进行适当的润色使其更通顺。\n",
    "    3.  **最终输出字段定义**:\n",
    "        - `original_text`: **必须**原封不动地使用 `Rough Chinese Transcript` 中的中文文本。\n",
    "        - `translated_text`: **必须**是你执行【核心原则 #2】后，全新生成的高质量中文翻译。\n",
    "    4.  **忽略微小差异**: 在对齐过程中，可以忽略不影响核心语义的语气词 (如 `ええと`, `あの`) 或助词差异。\n",
    "    5.  **处理对话**: 如果一个片段中包含不同说话人的对话，请在 `original_text` 以及 `translated_text` 中使用**三个半角空格 `   `** 来分隔。\n",
    "\n",
    "    # 你的工作流程\n",
    "    1.  **迭代与定位 (Cross-Lingual Lookup)**: 遍历 `Rough Chinese Transcript` 中的每一个片段。根据其**中文文本的含义**，在 `Precise Japanese Transcript` 中找到语义上对应的**日文原文段落**。\n",
    "    2.  **决策与翻译 (Decision & Translation)**: 应用【核心原则 #2】。判断你找到的日文原文是否可用。\n",
    "        - **如果日文可用**: 将这段**日文**翻译成新的、高质量的中文。\n",
    "        - **如果日文不可用**: 将原始的**粗糙中文**进行润色或直接采用，作为最终翻译。\n",
    "    3.  **封装 (Packaging)**: 将结果封装成JSON对象。`start`和`end`来自输入，`original_text`来自输入的粗糙中文，`translated_text`是你上一步新生成的翻译。\n",
    "    4.  **自我纠错与验证**: 在输出前，必须执行内部检查：\n",
    "        - **行数一致性检查**: 最终输出的片段数量，**必须**与输入的 `Rough Chinese Transcript` 的片段数量完全一致。此处应为{BATCH_SIZE}个片段。\n",
    "        - **语义一致性检查**: 你的 `translated_text` 的含义，必须与输入的 `original_text` 的含义高度相关。如果无关，说明定位出错，必须重试。\n",
    "    5.  **最终输出格式**: 你的回答**必须**是一个单独的JSON对象，且只包含一个名为 `segments` 的键，其值为一个JSON数组。每个JSON对象包含以下字段：'start', 'end', 'original_text' (粗转录结果里的中文), 'translated_text' (精确转录结果翻译后的中文)。\n",
    "    \n",
    "    ---\n",
    "    ### 【示例学习区】\n",
    "    ---\n",
    "\n",
    "    #### 示例 1: 理想情况 (使用精翻日文)\n",
    "\n",
    "    **[输入]**\n",
    "    -   **Rough Chinese Transcript (片段)**:\n",
    "        ```json\n",
    "        [\n",
    "        {{\"start\": 75.714, \"end\": 80.818, \"text\": \"请告诉我们您的名字我是夏目ひびき\"}},\n",
    "        {{\"start\": 82.466, \"end\": 87.122, \"text\": \"您是从事这行业多久了呢？五十年了\"}}\n",
    "        ]\n",
    "        ```\n",
    "    -   **Precise Japanese Transcript (相关部分)**:\n",
    "        ```text\n",
    "        ...はい、夏目響です。デビューして何年目ぐらいですか？...\n",
    "        ```\n",
    "\n",
    "    **[模型思考过程]**\n",
    "    1.  **处理片段1**: 粗糙中文是 \"请告诉我们您的名字我是夏目ひびき\"。我在日文精确文本中找到了对应的 `はい、夏目響です。`。这段日文质量很高。\n",
    "    2.  **决策**: 我将翻译这段高质量的日文。`はい、夏目響です。` -> \"是的，我是夏目响。\"\n",
    "    3.  **封装片段1**: `original_text` 使用输入的粗糙中文，`translated_text` 使用我的新翻译。\n",
    "    4.  **处理片段2**: 粗糙中文是 \"您是从事这行业多久了呢？五十年了\"。我在日文精确文本中找到了对应的 `デビューして何年目ぐらいですか？`。这段日文质量很高。\n",
    "    5.  **决策**: 我将翻译这段高质量的日文。`デビューして何年目ぐらいですか？` -> \"你出道几年了？\"\n",
    "    6.  **封装片段2**: `original_text` 使用输入的粗糙中文，`translated_text` 使用我的新翻译。\n",
    "    7.  **检查**: 行数和语义都一致。最终格式正确。\n",
    "\n",
    "    **[输出]**\n",
    "    ```json\n",
    "    {{\n",
    "    \"segments\": [\n",
    "        {{\n",
    "        \"start\": 75.714,\n",
    "        \"end\": 80.818,\n",
    "        \"original_text\": \"请告诉我们您的名字我是夏目ひびき\",\n",
    "        \"translated_text\": \"是的，我是夏目响。\"\n",
    "        }},\n",
    "        {{\n",
    "        \"start\": 82.466,\n",
    "        \"end\": 87.122,\n",
    "        \"original_text\": \"您是从事这行业多久了呢？五十年了\",\n",
    "        \"translated_text\": \"你出道几年了？\"\n",
    "        }}\n",
    "    ]\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    ---\n",
    "\n",
    "    #### 示例 2: 回退情况 (精确日文不可用)\n",
    "\n",
    "    **[输入]**\n",
    "    -   **Rough Chinese Transcript (片段)**:\n",
    "        ```json\n",
    "        [\n",
    "        {{\"start\": 331.266, \"end\": 336.478, \"text\": \"想要做H的都市传说\"}}\n",
    "        ]\n",
    "        ```\n",
    "    -   **Precise Japanese Transcript (相关部分)**:\n",
    "        ```text\n",
    "        ...まあAVですからね、一応、エッチな方が自然かなって...（ノイズ）...うん...あの...それで...\n",
    "        ```\n",
    "\n",
    "    **[模型思考过程]**\n",
    "    1.  **处理片段1**: 粗糙中文是 \"想要做H的都市传说\"。我根据这个意思在日文精确文本中定位，发现对应的部分是 `...（ノイズ）...うん...あの...それで...` (噪音...嗯...那个...所以...)。\n",
    "    2.  **决策**: 这段日文是无意义的噪音和填充词，质量极差。**我必须执行回退策略**。\n",
    "    3.  **回退翻译**: 我将直接采用输入的粗糙中文 \"想要做H的都市传说\" 作为翻译基础，可以稍作润色，比如 \"想做个色色的都市传说\"。\n",
    "    4.  **封装片段1**: `original_text` 使用输入的粗糙中文，`translated_text` 使用我回退后润色的中文。\n",
    "    5.  **检查**: 行数和语义都一致。最终格式正确。\n",
    "\n",
    "    **[输出]**```json\n",
    "    {{\n",
    "    \"segments\": [\n",
    "        {{\n",
    "        \"start\": 331.266,\n",
    "        \"end\": 336.478,\n",
    "        \"original_text\": \"想要做H的都市传说\",\n",
    "        \"translated_text\": \"想做有关有点色情的都市传说的事。\"\n",
    "        }}\n",
    "    ]\n",
    "    }}\n",
    "    ```\n",
    "    \n",
    "    {style_guide}\n",
    "    \"\"\"\n",
    "\n",
    "    user_input = f\"\"\"\n",
    "    请处理以下数据：\n",
    "\n",
    "    --- 带时间戳的不精确转录本 (当前批次) ---\n",
    "    {formatted_timings}\n",
    "    ---------------------------------\n",
    "\n",
    "    --- 精确转录本 (相关片段) ---\n",
    "    {accurate_text_chunk}\n",
    "    -------------------\n",
    "\n",
    "    请严格按照指示，完成对齐和翻译，并仅返回JSON格式的输出。\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"正在调用OpenAI API处理当前批次，请稍候...\")\n",
    "    \n",
    "    try:\n",
    "        rsp = client.responses.create(\n",
    "            model=MODEL_FOR_ALIGNMENT,\n",
    "            instructions=instructions,\n",
    "            input=user_input,\n",
    "            text={                    # 这里才是关键\n",
    "            \"format\": {\n",
    "            \"type\":   \"json_schema\",\n",
    "            \"name\":   \"aligned_segments\",\n",
    "            \"schema\": ALIGN_SCHEMA,   # 你的 JSON Schema\n",
    "            \"strict\": True            # 建议打开严格模式\n",
    "        }\n",
    "    }\n",
    "        )\n",
    "        out_obj  = json.loads(rsp.output_text)   # rsp.output_text 是字符串\n",
    "        seg_list = out_obj[\"segments\"]           # 取出真正的字幕列表\n",
    "\n",
    "        return seg_list                          # 主流程依然收到 list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"调用OpenAI API时发生错误: {e}\")\n",
    "        return None\n",
    "# --- SRT生成函数 (来自你的代码，做了一点小优化) ---\n",
    "\n",
    "def format_time_for_srt(sec):\n",
    "    if sec is None:\n",
    "        sec = 0.0\n",
    "    h, rem = divmod(sec, 3600)\n",
    "    m, s = divmod(rem, 60)\n",
    "    ms = int((s - int(s)) * 1000)\n",
    "    return f\"{int(h):02}:{int(m):02}:{int(s):02},{ms:03}\"\n",
    "\n",
    "def to_srt(data):\n",
    "    lines = []\n",
    "    for i, item in enumerate(data, 1):\n",
    "        if not {\"start\", \"end\", \"translated_text\"} <= item.keys():\n",
    "            print(f\"警告: 第 {i} 条数据缺字段，已跳过\")\n",
    "            continue\n",
    "        lines += [\n",
    "            str(i),\n",
    "            f\"{format_time_for_srt(item['start'])} --> {format_time_for_srt(item['end'])}\",\n",
    "            item[\"translated_text\"],\n",
    "            \"\",\n",
    "        ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# --- 主逻辑函数 (重构后) ---\n",
    "\n",
    "def main():\n",
    "    # --- 文件路径 ---\n",
    "    json_file      = 'vocals.json'\n",
    "    accurate_file  = f\"transcripts/{wav_path.stem}.txt\" \n",
    "    output_srt     = f'srts/{wav_path.stem}.srt'\n",
    "\n",
    "    os.makedirs('srts', exist_ok=True)  # 确保输出目录存在\n",
    "\n",
    "    timing_data, accurate_text = load_data(json_file, accurate_file)\n",
    "\n",
    "    if not (timing_data and accurate_text):\n",
    "        return\n",
    "\n",
    "    segments = timing_data.get(\"segments\", [])\n",
    "    if not segments:\n",
    "        print(\"segments 为空\")\n",
    "        return\n",
    "\n",
    "    total_batches = (len(segments) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    all_results   = []\n",
    "\n",
    "    for batch_idx in range(total_batches):\n",
    "        batch = segments[batch_idx * BATCH_SIZE : (batch_idx + 1) * BATCH_SIZE]\n",
    "        if not batch:\n",
    "            continue\n",
    "        print(f\"\\n=== 处理批次 {batch_idx + 1}/{total_batches} ===\")\n",
    "\n",
    "        # A. 提取精确文本片段\n",
    "        chunk = extract_relevant_chunk_with_api(accurate_text, batch)\n",
    "        if not chunk:\n",
    "            print(\"  无法提取片段，跳过\")\n",
    "            continue\n",
    "\n",
    "        # B. 对齐 + 翻译\n",
    "        timing_prompt = format_timing_data_for_prompt(batch)\n",
    "        result = align_and_translate(timing_prompt, chunk)\n",
    "        if result:\n",
    "            all_results.extend(result)\n",
    "            print(f\"  成功获得 {len(result)} 条字幕\")\n",
    "        else:\n",
    "            print(\"  对齐/翻译失败\")\n",
    "\n",
    "        time.sleep(1)  # 轻微延时减小速率限制风险\n",
    "\n",
    "    if all_results:\n",
    "        Path(output_srt).write_text(to_srt(all_results), encoding=\"utf-8\")\n",
    "        print(f\"\\n✅ 已生成 SRT: {output_srt}\")\n",
    "    else:\n",
    "        print(\"\\n❌ 未获得任何有效字幕\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subworkflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
