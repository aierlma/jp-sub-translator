{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855677f2",
   "metadata": {},
   "source": [
    "# First step\n",
    "We use demucs to separate vocals\n",
    "\n",
    "# Second\n",
    "\n",
    "We get a rough timestamp(using kotoba-tech/kotoba-whisper-v2.0-faster, 4m processing for a 2h movie) and an accurate transcript without timestamp(using [amine whisper](https://huggingface.co/litagin/anime-whisper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0585bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "import os\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c601ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio(input_video: str,\n",
    "                  sample_rate: int = 16000,\n",
    "                  channels: int = 1,\n",
    "                  codec: str = \"pcm_s16le\") -> Path:\n",
    "    \"\"\"\n",
    "    从任意视频文件中提取音频，输出 WAV 并返回输出文件路径。\n",
    "    文件名：保留原始 stem，后缀改为 .wav\n",
    "    \"\"\"\n",
    "    in_path = Path(input_video)\n",
    "    stem = in_path.stem\n",
    "    audio_output = in_path.with_name(f\"{stem}.wav\")\n",
    "\n",
    "    cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-y\",  # 如果输出已存在则覆盖\n",
    "        \"-i\", str(in_path),\n",
    "        \"-vn\",\n",
    "        \"-acodec\", codec,\n",
    "        \"-ar\", str(sample_rate),\n",
    "        \"-ac\", str(channels),\n",
    "        str(audio_output)\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "    return audio_output\n",
    "\n",
    "\n",
    "def separate_vocals(audio_input: Path,\n",
    "                    stems: str = \"vocals\",\n",
    "                    model: str = \"htdemucs\") -> None:\n",
    "    \"\"\"\n",
    "    对提取后的音频做人声分离，两声道模式下只保留 vocals。\n",
    "    Demucs 会在当前目录下创建一个 separated/<model>/<stem> 文件夹。\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"demucs\",\n",
    "        f\"--two-stems={stems}\",\n",
    "        \"-n\", model,\n",
    "        str(audio_input)\n",
    "    ]\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "\n",
    "video_path = Path(\"D:/Downloads/AAA.mp4\")\n",
    "\n",
    "# 提取音频，自动生成 your_movie.wav\n",
    "wav_path = extract_audio(video_path,\n",
    "                            sample_rate=16000,\n",
    "                            channels=1)\n",
    "\n",
    "# 基于 your_movie.wav 分离人声\n",
    "demucs_model = \"htdemucs\" \n",
    "separate_vocals(wav_path,\n",
    "                stems=\"vocals\",\n",
    "                model=demucs_model)\n",
    "\n",
    "# 分离后的人声文件路径\n",
    "vocals_path = Path(f\"separated/{demucs_model}/{wav_path.stem}/vocals.wav\")\\\n",
    "\n",
    "print(f\"分离后的人声文件路径: {vocals_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf9a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\n",
    "    \"language\": \"Japanese\",\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "}\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"./models/anime\",\n",
    "    device=\"cuda\",\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "    chunk_length_s=30.0,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "result = pipe(vocals_path, generate_kwargs=generate_kwargs)\n",
    "\n",
    "os.makedirs(\"transcripts\", exist_ok=True)\n",
    "with open(f\"transcripts/{wav_path.stem}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(result[\"text\"])\n",
    "\n",
    "\n",
    "\n",
    "out_dir = Path(f\"timestamps/{wav_path.stem}\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "cmd = [\n",
    "    \"whisperx\", str(vocals_path),\n",
    "    \"--model\", \"tiny\",\n",
    "    \"--device\", \"cuda\",\n",
    "    \"--language\", \"ja\",\n",
    "    \"--vad_method\", \"silero\",\n",
    "    \"--chunk_size\", \"6\",\n",
    "    \"--batch_size\", \"16\",\n",
    "    \"--compute_type\", \"float16\",\n",
    "    # \"--output_format\", \"json\",\n",
    "    \"--highlight_words\", \"True\",\n",
    "    \"--output_dir\", str(out_dir)\n",
    "]\n",
    "subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb04fcb",
   "metadata": {},
   "source": [
    "# Third step\n",
    "Use openai API to align the trancription to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"]) # Ensure you have set your OpenAI API key in the environment variable or .env file\n",
    "\n",
    "\n",
    "def load_data(json_path, accurate_text_path):\n",
    "    \"\"\"加载JSON时间戳数据和精确的文本文件\"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            timing_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: JSON文件未找到 at {json_path}\")\n",
    "        return None, None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"错误: JSON文件格式不正确 at {json_path}\")\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        with open(accurate_text_path, 'r', encoding='utf-8') as f:\n",
    "            accurate_text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 精确文本文件未找到 at {accurate_text_path}\")\n",
    "        return None, None\n",
    "        \n",
    "    return timing_data, accurate_text\n",
    "\n",
    "def format_timing_data_for_prompt(timing_data):\n",
    "    \"\"\"将JSON数据格式化为更易于LLM阅读的字符串\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for segment in timing_data.get(\"segments\", []):\n",
    "        start = segment.get(\"start\")\n",
    "        end = segment.get(\"end\")\n",
    "        text = segment.get(\"text\")\n",
    "        formatted_string += f\"  - 时间: {start:.3f} - {end:.3f}, 文本: \\\"{text}\\\"\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "def align_and_translate(formatted_timings, accurate_text):\n",
    "    \"\"\"\n",
    "    使用OpenAI API来对齐和翻译文本。\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    你是一个专业的字幕制作和翻译专家。你的任务是根据两份日文转录本，完成一个精确的、带时间戳的中文翻译。\n",
    "    一份是“带时间戳的不精确转录本”，它提供了准确的时间信息但文本内容可能有误。\n",
    "    另一份是“精确转录本”，它提供了准确的文本内容但没有时间信息。\n",
    "\n",
    "    你的工作流程如下：\n",
    "    1.  **对齐**：以“带时间戳的不精确转录本”为基础框架，将“精确转录本”中的文本内容，智能地填充到对应的时间段（segment）中。注意，两者的句子和词语不一定完全匹配，但是大部分是match的，尤其是一个segment里的第一个假名和最后一个假名，这是你匹配的重要参考，千万不能错配，你需要根据语义和上下文进行最佳的对齐，确保最终的文本流畅且符合逻辑，最重要的是，精细文本与粗文本的时间戳是一致的。\n",
    "    2.  **翻译**：将对齐好的、精确的日文文本内容，逐段翻译成流畅、自然的简体中文。\n",
    "    3.  **输出**：必须以一个JSON数组的格式返回结果，不包含任何额外的解释。每个JSON对象包含以下字段：'start', 'end', 'original_text' (对齐后的精确日文), 'translated_text' (翻译后的中文)。\n",
    "    \n",
    "    备注：\n",
    "    有时精确转录本中可能出现一些语气词在粗转录本中没有的情况，或者相反，这种情况下，忽略这些语气词，只对齐粗转录本中存在的内容。\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    请处理以下数据：\n",
    "\n",
    "    --- 带时间戳的不精确转录本 ---\n",
    "    {formatted_timings}\n",
    "    ---------------------------------\n",
    "\n",
    "    --- 精确转录本 ---\n",
    "    {accurate_text}\n",
    "    -------------------\n",
    "\n",
    "    请严格按照指示，完成对齐和翻译，并仅返回JSON格式的输出。\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"正在调用OpenAI API进行处理，请稍候...\")\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            # 推荐使用 gpt-4-turbo 或 gpt-4o\n",
    "            model=\"gpt-4.1\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            # 开启JSON模式，确保返回格式正确\n",
    "            response_format={\"type\": \"json_object\"} \n",
    "        )\n",
    "        \n",
    "        # API返回的内容在 response.choices[0].message.content 中\n",
    "        # 它应该是一个JSON格式的字符串\n",
    "        response_content = response.choices[0].message.content\n",
    "        \n",
    "        # 解析这个JSON字符串\n",
    "        # GPT返回的JSON可能在外层有一个key，我们需要找到那个包含数组的key\n",
    "        response_json = json.loads(response_content)\n",
    "        \n",
    "        # 假设返回的JSON结构是 {\"subtitles\": [...]} 或直接是 [...]\n",
    "        # 我们需要找到那个列表\n",
    "        for key, value in response_json.items():\n",
    "            if isinstance(value, list):\n",
    "                return value\n",
    "        \n",
    "        # 如果模型直接返回了一个列表（虽然不太可能在JSON模式下）\n",
    "        if isinstance(response_json, list):\n",
    "            return response_json\n",
    "\n",
    "        print(\"错误：无法在API响应中找到字幕列表。\")\n",
    "        return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"调用OpenAI API时发生错误: {e}\")\n",
    "        return None\n",
    "\n",
    "def to_srt(data):\n",
    "    \"\"\"将处理后的数据转换为SRT字幕格式的字符串\"\"\"\n",
    "    srt_content = \"\"\n",
    "    for i, item in enumerate(data, 1):\n",
    "        start_time = item['start']\n",
    "        end_time = item['end']\n",
    "        \n",
    "        # 格式化时间戳，例如 00:01:02,345\n",
    "        def format_time(s):\n",
    "            hours, remainder = divmod(s, 3600)\n",
    "            minutes, seconds = divmod(remainder, 60)\n",
    "            milliseconds = int((seconds - int(seconds)) * 1000)\n",
    "            return f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02},{milliseconds:03}\"\n",
    "\n",
    "        start_srt = format_time(start_time)\n",
    "        end_srt = format_time(end_time)\n",
    "        \n",
    "        # 你可以选择只放中文，或者中日双语\n",
    "        original = item['original_text']\n",
    "        translated = item['translated_text']\n",
    "        \n",
    "        srt_content += f\"{i}\\n\"\n",
    "        srt_content += f\"{start_srt} --> {end_srt}\\n\"\n",
    "        srt_content += f\"{translated}\\n\" # 只显示中文\n",
    "        # srt_content += f\"{translated}\\n{original}\\n\" # 如果需要双语\n",
    "        srt_content += \"\\n\"\n",
    "        \n",
    "    return srt_content\n",
    "\n",
    "def main():\n",
    "    # --- 文件路径 ---\n",
    "    json_file_path = Path(f'timestamps/{wav_path.stem}/vocals.json')  \n",
    "    accurate_text_file_path = f\"transcripts/{wav_path.stem}.txt\" \n",
    "    output_srt_path = f'srts/{wav_path.stem}.srt'\n",
    "\n",
    "    # 1. 加载数据\n",
    "    timing_data, accurate_text = load_data(json_file_path, accurate_text_file_path)\n",
    "    if not timing_data or not accurate_text:\n",
    "        return\n",
    "\n",
    "    # 2. 格式化数据以用于Prompt\n",
    "    formatted_timings = format_timing_data_for_prompt(timing_data)\n",
    "\n",
    "    # 3. 调用API进行对齐和翻译\n",
    "    aligned_data = align_and_translate(formatted_timings, accurate_text)\n",
    "    \n",
    "    if aligned_data:\n",
    "        print(\"\\n--- API返回的对齐和翻译结果 ---\")\n",
    "        print(json.dumps(aligned_data, indent=2, ensure_ascii=False))\n",
    "        \n",
    "        # 4. 转换为SRT格式\n",
    "        srt_output = to_srt(aligned_data)\n",
    "        \n",
    "        # 5. 保存到文件\n",
    "        with open(output_srt_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(srt_output)\n",
    "            \n",
    "        print(f\"\\n成功！SRT字幕文件已保存到: {output_srt_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subworkflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
